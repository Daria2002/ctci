#include <iostream>

/**
 * Design a system like Pastebin, where a user can enter a piece 
 * of text and get a randomlygenerated URL to access it.
*/
int main() {
    std::cout << "Step 1: Scope of this problem\n"
                 "  -No user accounts and editing docs.\n"
                 "  -Tracks analytics of how many times each page is accessed.\n"
                 "  -Old docs get deleted after not being accessed for a sufficiently long period of time.\n"
                 "  -Document URL should not be easily guessed.\n"
                 "  -The system has a frontend as well as API.\n"
                 "  -The analytics for each URL can be accessed through a stats link on each page.\n"
                 "Step 2: Make reasonable assumptions\n"
                 "  -the system gets heavy traffic and contains many millions of docs.\n"
                 "  -traffic is not equally distributed across docs. Some docs get much more access than others.\n"
                 "Step 3: Draw the major components\n"
                 "  -documents can be stored in db or on a file. It's probably better to store them on a file because\n"
                 "   documents can be large and searching capabilities won't be needed.\n"
                 "  -There will be a simple db that looks up the location of each file and then access the file.\n"
                 "  -There will also be a db that tracks analytics. It will add each visit as a row in a db and\n"
                 "   than to access the stats of each visit, we pull the relevant data in from this db.\n"
                 "Step 4: Identify the key issues\n"
                 "  -reading data from the filesystem is relatively slow compared with reading from data in memory.\n"
                 "  -there should be a cache to store some docs that are much more frequently accessed than others.\n"
                 "  -since there is no option to edit docs there is no need to worry about invalidating this cache.\n"
                 "  -for retrieving docs from db introduce sharding the database using some mapping from the url\n"
                 "   (i.e. the url's hash code modulo some integer), which will allow us to quickly locate the db which\n"
                 "   contains this file. There is also approach where we could skip the db entirely and just let a hash\n"
                 "   of the URL indicate which server contains the doc. Potential issue: adding servers - difficult to\n"
                 "   redistribute docs. Generating URLs should provide difficult access without being provided the link.\n"
                 "  -one approach for generating URL is to generate a random GUID (globally unique identifier)-128-bit value\n"
                 "  -GUID is not strictly guaranteed to be unique but it has low enough odds of collision that we can treat it\n"
                 "   as unique. The drawback of this is that such a URL is not very pretty to the user. We could hash it to a\n"
                 "   smaller value, but then that increases the odds of collision. The second approach could be generating\n"
                 "   10-character sequence of letters and number, which gives 36^10 possible strings. Even with a billion URLs,\n"
                 "   the odds of collision on specific URL are very low. When collision occurs, we can just generate a new URL.\n"
                 "  -collisions are rare enough, so lazy approach (detect collision and retry) is sufficient\n"
                 "  -analytics contains of displaying the number of visits and possibly location and time.\n"
                 "  -there are 2 options: store the raw data from each visit or store just the data we'll use (i.e.num of visits)\n"
                 "  -better: raw data, we never know what features we'll add to the analytics down the road. Raw data -> flexibility\n"
                 "  -Raw data doesn't need to be easily searchable or even accessible. It can be store in a log and sent to the server.\n"
                 "  -Issue: substantial amount of data. Solution: storing data probabilistically. Each URL would have storage_probability\n"
                 "   associated with it. As the popularity goes up, the storage_probability goes down. It means that for a\n"
                 "   popular document we would log data only one out of ten times, at random. When we look up the number of visits\n"
                 "   we'll need to adjust it based on the probability (for example, multiplying it by 10). This will lead to the small\n"
                 "   inaccuracy, but that may be acceptable. There is also other approach where we don't use log files because they are\n"
                 "   not designed to be used frequently. A database is a series of bytes that is managed by a database management system\n"
                 "   (DBMS). A file is a series of bytes that is managed by a file system. Thus, any database or file is a series of bytes\n"
                 "   that, once stored, is called a data store. We could add separate db to store URL, time and visits. Every time a URL\n"
                 "   is visited, we can increment the appropriate row and column. This datastore can also be sharded by the URL. We can\n"
                 "   introduce the cache for the stats for the most popular URLs.\n";
}
